{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "argv": [
        "python",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3",
      "env": null,
      "interrupt_mode": "signal",
      "language": "python",
      "metadata": null,
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "name": "assignment.ipynb",
    "colab": {
      "name": "Copy of assignment.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xv-A24Hl8JFT"
      },
      "source": [
        "# Assignment 3: Word2Vec\n",
        "\n",
        "In this assignment, we will see how we can use Word2Vec (or any similar word embedding) to use information from unlabelled data to help us classify better!\n",
        "\n",
        "You will be using the sentiment data from last week, either the yelps or movies, whichever you wish. \n",
        "\n",
        "Your goal will be to simulate the following situation: you have a **small** set of labelled data and a large set of unlabelled data. Show how the two follow 2 techniques compare as the amount of labelled data increases. You should train them on the small labelled subset and test their performance on the rest of the data. \n",
        "\n",
        "In other words, train on 1k, test on 99k. Then train on 2k, test on 98k. Then train on 4k, test on 96k. Etc.\n",
        "\n",
        "1. Logistic regression trained on labelled data, documents represented as term-frequency matrix of your choice. You can learn the vocabulary from the entire dataset or only the labelled data.\n",
        "\n",
        "2. Logistic regression trained on the labelled data, documents represented as word2vec vectors where you train word2vec using the entire dataset. Play around with different settings of word2vec (training window size, K-negative, skip-gram vs BOW, training windows, etc.). Note: we didn't go over the options in detail in class, so you will need to read about them a bit!\n",
        "\n",
        "You can read about the gensime word2vec implementation [here](https://radimrehurek.com/gensim/models/word2vec.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UClxOo3zd7SI"
      },
      "source": [
        "## Import & Split & Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YIsk-d23cTG",
        "outputId": "aae32acf-713d-4d75-8e3f-9b6be17b262b"
      },
      "source": [
        "# Run this if running in Google Collab\n",
        "# Mount google drive if running from Google Collab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set current directory if running from Google Collab\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks')# here use your path to current notebook"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BadU4zLu38lF"
      },
      "source": [
        "#import auxiliar functions\n",
        "import os,sys,inspect\n",
        "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
        "parentdir = os.path.dirname(os.path.dirname(currentdir))\n",
        "sys.path.insert(1, parentdir)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFV7h_BtuUTe"
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "# EXERCISE 4\n",
        "#\n",
        "# Prove Ng and Jordan right!!!\n",
        "\n",
        "yelps = pd.read_csv('yelps.csv').fillna(' ')\n",
        "imdb = pd.read_csv('movies.csv').fillna(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klpvaSSHEmYz",
        "outputId": "f76d15de-95f0-4a19-e82f-f46a6caba5dc"
      },
      "source": [
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "#lemmitizing with spacy takes to long, so we just do:\n",
        "def preprocess(X):\n",
        "  documents = []\n",
        "  APPO = {\n",
        "  \"aren't\" : \"are not\",\n",
        "  \"can't\" : \"cannot\",\n",
        "  \"couldn't\" : \"could not\",\n",
        "  \"didn't\" : \"did not\",\n",
        "  \"doesn't\" : \"does not\",\n",
        "  \"don't\" : \"do not\",\n",
        "  \"hadn't\" : \"had not\",\n",
        "  \"hasn't\" : \"has not\",\n",
        "  \"haven't\" : \"have not\",\n",
        "  \"he'd\" : \"he would\",\n",
        "  \"he'll\" : \"he will\",\n",
        "  \"he's\" : \"he is\",\n",
        "  \"i'd\" : \"I would\",\n",
        "  \"i'd\" : \"I had\",\n",
        "  \"i'll\" : \"I will\",\n",
        "  \"i'm\" : \"I am\",\n",
        "  \"isn't\" : \"is not\",\n",
        "  \"it's\" : \"it is\",\n",
        "  \"it'll\":\"it will\",\n",
        "  \"i've\" : \"I have\",\n",
        "  \"let's\" : \"let us\",\n",
        "  \"mightn't\" : \"might not\",\n",
        "  \"mustn't\" : \"must not\",\n",
        "  \"shan't\" : \"shall not\",\n",
        "  \"she'd\" : \"she would\",\n",
        "  \"she'll\" : \"she will\",\n",
        "  \"she's\" : \"she is\",\n",
        "  \"shouldn't\" : \"should not\",\n",
        "  \"that's\" : \"that is\",\n",
        "  \"there's\" : \"there is\",\n",
        "  \"they'd\" : \"they would\",\n",
        "  \"they'll\" : \"they will\",\n",
        "  \"they're\" : \"they are\",\n",
        "  \"they've\" : \"they have\",\n",
        "  \"we'd\" : \"we would\",\n",
        "  \"we're\" : \"we are\",\n",
        "  \"weren't\" : \"were not\",\n",
        "  \"we've\" : \"we have\",\n",
        "  \"what'll\" : \"what will\",\n",
        "  \"what're\" : \"what are\",\n",
        "  \"what's\" : \"what is\",\n",
        "  \"what've\" : \"what have\",\n",
        "  \"where's\" : \"where is\",\n",
        "  \"who'd\" : \"who would\",\n",
        "  \"who'll\" : \"who will\",\n",
        "  \"who're\" : \"who are\",\n",
        "  \"who's\" : \"who is\",\n",
        "  \"who've\" : \"who have\",\n",
        "  \"won't\" : \"will not\",\n",
        "  \"wouldn't\" : \"would not\",\n",
        "  \"you'd\" : \"you would\",\n",
        "  \"you'll\" : \"you will\",\n",
        "  \"you're\" : \"you are\",\n",
        "  \"you've\" : \"you have\",\n",
        "  \"'re\": \" are\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'll\":\" will\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"tryin'\":\"trying\"\n",
        "  }\n",
        "  stemmer = WordNetLemmatizer()\n",
        "  for sen in range(0, len(X)):\n",
        "    document = re.sub(r'\\W', ' ', str(X.iloc[sen])) # Remove all the special characters\n",
        "    document = [APPO[word] if word in APPO else word for word in document.split()]\n",
        "    document = ' '.join(document)\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document) # remove all single characters\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)  # Remove single characters from the start\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I) # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'^b\\s+', '', document) # Removing prefixed 'b'\n",
        "    document = document.lower() # Converting to Lowercase\n",
        "    document = document.split() # Lemmatization\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    document = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', document) #Removes numbers and words containing numbers\n",
        "\n",
        "    documents.append(document)\n",
        "  return documents\n",
        "\n",
        "df = pd.concat([yelps[['positive', 'text']], imdb[['positive', 'text']]])\n",
        "df = df.reset_index(drop=True)\n",
        "msk1 = np.random.rand(len(df)) < 0.01\n",
        "msk2 = np.random.rand(len(df)) < 0.02\n",
        "msk4 = np.random.rand(len(df)) < 0.04\n",
        "\n",
        "train1 = df[msk1]\n",
        "test99 = df[~msk1]\n",
        "train2 = df[msk2]\n",
        "test98 = df[~msk2]\n",
        "train4 = df[msk4]\n",
        "test96 = df[~msk4]\n",
        "\n",
        "\n",
        "corpus1 = preprocess(train1.text)\n",
        "y1 = train1.positive\n",
        "corpus_test99 = preprocess(test99.text)\n",
        "y_test99 = test99.positive\n",
        "\n",
        "corpus2 = preprocess(train2.text)\n",
        "y2 = train2.positive\n",
        "corpus_test98 = preprocess(test98.text)\n",
        "y_test98 = test98.positive\n",
        "\n",
        "corpus4 = preprocess(train4.text)\n",
        "y4 = train4.positive\n",
        "corpus_test96 = preprocess(test96.text)\n",
        "y_test96 = test96.positive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOQwzlwjRS_f",
        "outputId": "ae6f0f95-71ae-4611-91ad-c216ec261213"
      },
      "source": [
        "print(len(y1))\n",
        "print(len(corpus1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1611\n",
            "1611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MauKG5n7Sus2",
        "outputId": "2ff150b2-71b6-4103-a66e-1d5a3b50f0d2"
      },
      "source": [
        "print(len(yelps))\n",
        "print(len(imdb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100003\n",
            "50003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJxiIiiVEcLK"
      },
      "source": [
        "#Logestic Regression w/o word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq_sCHDsPQDI"
      },
      "source": [
        "We can test different feature extraction methods and different classifiers to improve fitting and we can tune hyperparameters. Since running pipelines with different classifiers takes a lot of time, we just tune hyperparameters for Tfidfvectorizer and LogesticRegression here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYR2A6spPyox",
        "outputId": "450d28f0-fc8d-4c13-97d4-fa1fcf8b4a5a"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "# Pipeline & Gridsearch setup\n",
        "# TFIDF pipeline setup\n",
        "tvc_pipe = Pipeline([\n",
        " ('tvec', TfidfVectorizer()),\n",
        " ('lr', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Setting params for TFIDF Vectorizer gridsearch\n",
        "tf_params = {\n",
        " 'tvec__min_df':[2,5,10,12,15,18,20],\n",
        " 'tvec__max_df': [0.8,0.82, 0.85, 0.78, 0.75],\n",
        " 'tvec__norm': ['l2'],\n",
        " 'tvec__use_idf': [True],\n",
        " 'lr__penalty': ['l2'],\n",
        "}\n",
        "\n",
        "print('1% train and 99% test')\n",
        "# Setting up GridSearch for TFIDFVectorizer\n",
        "tvc_gs1 = GridSearchCV(tvc_pipe, param_grid=tf_params, cv = 5, verbose =1, n_jobs = 100)\n",
        "tvc_gs1.fit(corpus1, y1)\n",
        "# Scoring Training data on TFIDFVectorizer\n",
        "print(f'score for train data: {tvc_gs1.score(corpus1, y1)}')\n",
        "# Scoring Test data on TFIDFVectorizer\n",
        "print(f'score for test data: {tvc_gs1.score(corpus_test99, y_test99)}')\n",
        "\n",
        "print('2% train and 98% test')\n",
        "# Setting up GridSearch for TFIDFVectorizer\n",
        "tvc_gs2 = GridSearchCV(tvc_pipe, param_grid=tf_params, cv = 5, verbose =1, n_jobs = 100)\n",
        "tvc_gs2.fit(corpus2, y2)\n",
        "# Scoring Training data on TFIDFVectorizer\n",
        "print(f'score for train data: {tvc_gs2.score(corpus2, y2)}')\n",
        "# Scoring Test data on TFIDFVectorizer\n",
        "print(f'score for test data: {tvc_gs2.score(corpus_test98, y_test98)}')\n",
        "\n",
        "print('4% train and 96% test')\n",
        "# Setting up GridSearch for TFIDFVectorizer\n",
        "tvc_gs4 = GridSearchCV(tvc_pipe, param_grid=tf_params, cv = 5, verbose =1, n_jobs = 100)\n",
        "tvc_gs4.fit(corpus4, y4)\n",
        "# Scoring Training data on TFIDFVectorizer\n",
        "print(f'score for train data: {tvc_gs4.score(corpus4, y4)}')\n",
        "# Scoring Test data on TFIDFVectorizer\n",
        "print(f'score for test data: {tvc_gs4.score(corpus_test96, y_test96)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1% train and 99% test\n",
            "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=100)]: Using backend LokyBackend with 100 concurrent workers.\n",
            "[Parallel(n_jobs=100)]: Done 152 out of 175 | elapsed:  1.8min remaining:   16.3s\n",
            "[Parallel(n_jobs=100)]: Done 175 out of 175 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "score for train data: 0.9484792054624457\n",
            "score for test data: 0.8720711614272718\n",
            "2% train and 98% test\n",
            "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=100)]: Using backend LokyBackend with 100 concurrent workers.\n",
            "[Parallel(n_jobs=100)]: Done 152 out of 175 | elapsed:  1.2min remaining:   11.2s\n",
            "[Parallel(n_jobs=100)]: Done 175 out of 175 | elapsed:  1.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "score for train data: 0.9528519617540389\n",
            "score for test data: 0.8873398515373572\n",
            "4% train and 96% test\n",
            "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=100)]: Using backend LokyBackend with 100 concurrent workers.\n",
            "[Parallel(n_jobs=100)]: Done 152 out of 175 | elapsed:  2.5min remaining:   22.5s\n",
            "[Parallel(n_jobs=100)]: Done 175 out of 175 | elapsed:  2.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "score for train data: 0.9436177248677249\n",
            "score for test data: 0.9016240848025119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ4ap19-N2X7"
      },
      "source": [
        "#Logestic Regression with word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw7z_jcOEhEM"
      },
      "source": [
        "lst_corpus1 = []\n",
        "for i in corpus1:\n",
        "  lst_corpus1.append(i.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9niTwU6OCq7"
      },
      "source": [
        "from gensim.models import Word2Vec, word2vec\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "import sqlite3\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_features = 100    # Word vector dimensionality                      \n",
        "min_word_count = 40   # Minimum word count                        \n",
        "num_workers = 3       # Number of threads to run in parallel\n",
        "context = 10          # Context window size\n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "model = word2vec.Word2Vec(lst_corpus1, workers=num_workers, \\\n",
        "                size=num_features, min_count = min_word_count, \\\n",
        "                window = context, sample = downsampling)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik6IRNN2eSTr"
      },
      "source": [
        "def make_feature_vec(words, model, num_features):\n",
        "    \"\"\"\n",
        "    Average the word vectors for a set of words\n",
        "    \"\"\"\n",
        "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
        "    nwords = 0.\n",
        "    index2word_set = set(model.wv.index2word)  # words known to the model\n",
        "\n",
        "    for word in words:\n",
        "        if word in index2word_set: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vec = np.add(feature_vec,model[word])\n",
        "    \n",
        "    feature_vec = np.divide(feature_vec, nwords)\n",
        "    return feature_vec\n",
        "\n",
        "\n",
        "def get_avg_feature_vecs(reviews, model, num_features):\n",
        "    \"\"\"\n",
        "    Calculate average feature vectors for all reviews\n",
        "    \"\"\"\n",
        "    counter = 0.\n",
        "    review_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')  # pre-initialize (for speed)\n",
        "    \n",
        "    for review in reviews:\n",
        "        review_feature_vecs[int(counter)] = make_feature_vec(review, model, num_features)\n",
        "        counter = counter + 1.\n",
        "    return review_feature_vecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS33MIgpEoLo",
        "outputId": "dee04702-c071-4b85-8238-575e91e0dbe7"
      },
      "source": [
        "trainDataVecs1 = get_avg_feature_vecs(lst_corpus1, model, num_features=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqh5A5meeqFF"
      },
      "source": [
        "lst_corpus_test99 = []\n",
        "for i in corpus_test99:\n",
        "  lst_corpus_test99.append(i.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSWJq2o-His3",
        "outputId": "4ba04109-b578-48fa-e019-174a304bc9d2"
      },
      "source": [
        "testDataVecs99 = get_avg_feature_vecs(lst_corpus_test99, model, num_features=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx8caZzohxwv"
      },
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "knn = KNNImputer(n_neighbors=50)\n",
        "testDataVecs99 = knn.fit_transform(testDataVecs99)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BquMMh6xey3s",
        "outputId": "f1e31421-2255-4e7b-9780-34dcfaf65954"
      },
      "source": [
        "testDataVecs99.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(148395, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DabY5m6ie1qJ",
        "outputId": "09049ab9-d190-4fde-90ed-2c0bdc86ab96"
      },
      "source": [
        "np.isnan(testDataVecs99).any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmsKlzegfYuQ",
        "outputId": "87edd50d-ab48-441c-cfe2-c6b1576de381"
      },
      "source": [
        "lr1 = LogisticRegression()\n",
        "lr1.fit(trainDataVecs1, y1)\n",
        "result = lr1.predict(testDataVecs99)\n",
        "print(classification_report(y_test99, result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.68      0.67     74189\n",
            "           1       0.67      0.65      0.66     74206\n",
            "\n",
            "    accuracy                           0.67    148395\n",
            "   macro avg       0.67      0.67      0.67    148395\n",
            "weighted avg       0.67      0.67      0.67    148395\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pDYMo3hjCcS",
        "outputId": "a1c7b926-d739-4d18-966f-6ece49cd18d7"
      },
      "source": [
        "print(classification_report(y1, lr1.predict(trainDataVecs1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.70      0.69       817\n",
            "           1       0.68      0.67      0.67       794\n",
            "\n",
            "    accuracy                           0.68      1611\n",
            "   macro avg       0.68      0.68      0.68      1611\n",
            "weighted avg       0.68      0.68      0.68      1611\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}